{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# tensorboardX\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download scripts and data\n",
    "!mkdir data\n",
    "!mkdir data/images\n",
    "\n",
    "from format import print_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-47d673d38519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                            ]))\n\u001b[1;32m     11\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/images'"
     ]
    }
   ],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "dataset = ImageFolder(\"data/images\", transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataloader = Dataloader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(output_dim, input_dim, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Calculates padding given in output and input dim, and parameters of the convolutional layer\n",
    "\n",
    "    Arguments should all be integers. Use this function to calculate padding for 1 dimesion at a time.\n",
    "    Output dimensions should be the same or bigger than input dimensions\n",
    "\n",
    "    Returns 0 if invalid arguments were passed, otherwise returns an int or tuple that represents the padding.\n",
    "    \"\"\"\n",
    "\n",
    "    padding = (((output_dim - 1) * stride) - input_dim + kernel_size) // 2\n",
    "\n",
    "    if padding < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return padding\n",
    "\n",
    "def gen_block(input_channels, output_channels, kernel_size, stride, padding):\n",
    "    layers = []\n",
    "    layers += [nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "    layers += [nn.BatchNorm2d(output_channels)]\n",
    "    layers += [nn.ReLU(inplace=True)]\n",
    "    \n",
    "    return layers\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3, input_size=100, output_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = build_layers()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze()\n",
    "    \n",
    "    def build_layers(self):\n",
    "        layers = []\n",
    "        in_c = self.input_size\n",
    "        out_c = self.output_size * 8\n",
    "        \n",
    "        # dim: out_c x 4 x 4\n",
    "        layers += gen_block(in_c, out_c, 4, 1, 0)\n",
    "        in_c = out_c\n",
    "        out_c = self.output_size * 4\n",
    "        \n",
    "        # dim: out_c x 8 x 8\n",
    "        layers += gen_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.output_size * 2\n",
    "        \n",
    "        # dim: out_c x 16 x 16\n",
    "        layers += gen_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.output_size\n",
    "        \n",
    "        # dim: out_c x 32 x 32\n",
    "        layers += gen_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.channels\n",
    "        \n",
    "        # dim: out_c x 64 x 64\n",
    "        layers += [nn.ConvTranspose2d(in_c, out_c, 4, 2, 1), nn.Tanh()]\n",
    "        \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_block(input_channels, output_channels, kernel_size, stride, padding):\n",
    "    layers = []\n",
    "    layers += [nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "    layers += [nn.BatchNorm2d(output_channels)]\n",
    "    layers += [nn.LeakyReLU(0.2, inplace=True)]\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3, input_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = build_layers()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze()\n",
    "    \n",
    "    def build_layers(self):\n",
    "        layers = []\n",
    "        in_c = self.channels\n",
    "        out_c = self.input_dim\n",
    "        \n",
    "        # dim: out_c x 32 x 32\n",
    "        layers += discrim_block(in_c, out_c, 4, 1, 0)\n",
    "        in_c = out_c\n",
    "        out_c = self.input_dim * 2\n",
    "        \n",
    "        # dim: out_c x 16 x 16\n",
    "        layers += discrim_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.input_dim * 4\n",
    "        \n",
    "        # dim: out_c x 8 x 8\n",
    "        layers += discrim_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.input_dim * 8\n",
    "        \n",
    "        # dim: out_c x 4 x 4\n",
    "        layers += discrim_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.channels\n",
    "        \n",
    "        # dim: out_c x 64 x 64\n",
    "        layers += [nn.Conv2d(in_c, 1, 4, 1, 0), nn.Sigmoid()]\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.strftime(\"%a_%b_%d_%Y_%H:%M\", time.localtime())\n",
    "\n",
    "gen_input = 100\n",
    "gen_output = 64\n",
    "\n",
    "gen = Generator(input_size=gen_input, output_dim=gen_output)\n",
    "discrim = Discriminator()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device: {}\".format(device))\n",
    "gen.to(device)\n",
    "discrim.to(device)\n",
    "\n",
    "learn_rate = 0.001\n",
    "\n",
    "optG = optim.Adam(gen.parameters(), lr=learn_rate)\n",
    "optD = optim.Adam(discrim.parameters(), lr=learn_rate)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(gen_output, gen_input, 1, 1, device=device)\n",
    "\n",
    "real_label = 0.9\n",
    "fake_label = 0.1\n",
    "\n",
    "summary(gen, (gen_input))\n",
    "summary(discrim, (gen_output, gen_output, 3))\n",
    "writer = SummaryWriter('tensorboard_logs/run_{}'.format(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "print_step = 50\n",
    "\n",
    "gen_imgs = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    g_train_loss = 0\n",
    "    d_train_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        # Train Discriminator\n",
    "        \n",
    "        # only need images from data, don't care about class from ImageFolder\n",
    "        real_images = data[0].to(device)\n",
    "        b_size = real_images.size(0)\n",
    "        real_labels = torch.full((b_size,), real_label, device=device)\n",
    "        # get fake data from generator\n",
    "        noise = torch.randn(b_size, gen_input, 1, 1, device=device)\n",
    "        fake_images = gen(noise)\n",
    "        fake_labels = torch.full((b_size,), fake_label, device=device)\n",
    "        \n",
    "        # concat and shuffle real/fake images\n",
    "        images = torch.cat(real_images, fake_images, axis=0)\n",
    "        labels = torch.cat(real_labels, fake_labels, axis=0)\n",
    "        shuffle_i = np.arange(len(images))\n",
    "        np.random.shuffle(shuffle_i)\n",
    "        images = images[shuffle_i]\n",
    "        labels = labels[shuffle_i]\n",
    "        \n",
    "        # calculate loss and update gradients\n",
    "        discrim.zero_grad()\n",
    "        d_output = discrim(images).view(-1)\n",
    "        d_loss = criterion(d_output, labels)\n",
    "        d_loss.backward()\n",
    "        optD.step()\n",
    "        \n",
    "        d_train_loss += d_loss.item()\n",
    "        \n",
    "        # Train Generator\n",
    "        gen.zero_grad()\n",
    "        # get new output from discriminator for fake images\n",
    "        d_output = discrim(fake_images).view(-1)\n",
    "        # calculate the Generator's loss based on this, use real_labels since fake images are real for generator\n",
    "        g_loss = criterion(d_output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optG.step()\n",
    "        \n",
    "        g_train_loss += g_loss.item()\n",
    "        \n",
    "        if i % print_step == 0:\n",
    "            print_iter(curr_epoch=e, epochs=epochs, batch_i=i, num_batches=len(dataloader), d_loss=d_train_loss/(i+1), g_loss=g_train_loss/(i+1))\n",
    "    \n",
    "    print_iter(curr_epoch=e, epochs=epochs, writer=writer, d_loss=d_train_loss/(i+1), g_loss=g_train_loss/(i+1))\n",
    "    # save example images\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        fake_images = gen(fixed_noise).detach().cpu()\n",
    "    gen.train()\n",
    "    gen_imgs.append(vutils.make_grid(fake_images, padding=2, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
