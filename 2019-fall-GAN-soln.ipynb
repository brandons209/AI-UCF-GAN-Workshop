{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ImageFolder, DataLoader\n",
    "\n",
    "# tensorboardX\n",
    "from tensorboardX import SummaryWriter\n",
    "from format import print_iter, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download scripts and data\n",
    "\n",
    "from format import print_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "dataset = ImageFolder(\"data/images\", transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataloader = Dataloader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(output_dim, input_dim, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Calculates padding given in output and input dim, and parameters of the convolutional layer\n",
    "\n",
    "    Arguments should all be integers. Use this function to calculate padding for 1 dimesion at a time.\n",
    "    Output dimensions should be the same or bigger than input dimensions\n",
    "\n",
    "    Returns 0 if invalid arguments were passed, otherwise returns an int or tuple that represents the padding.\n",
    "    \"\"\"\n",
    "\n",
    "    padding = (((output_dim - 1) * stride) - input_dim + kernel_size) // 2\n",
    "\n",
    "    if padding < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return padding\n",
    "\n",
    "def gen_block(input_channels, output_channels, kernel_size, stride, padding):\n",
    "    layers = []\n",
    "    layers += [nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "    layers += [nn.BatchNorm2d(output_channels)]\n",
    "    layers += [nn.ReLU(inplace=True)]\n",
    "    \n",
    "    return layers\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3, input_size=100, output_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = build_layers()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze()\n",
    "    \n",
    "    def build_layers(self):\n",
    "        layers = []\n",
    "        in_c = self.input_size\n",
    "        out_c = self.output_size * 8\n",
    "        \n",
    "        # dim: out_c x 4 x 4\n",
    "        layers += gen_block(in_c, out_c, 4, 1, 0)\n",
    "        in_c = out_c\n",
    "        out_c = self.output_size * 4\n",
    "        \n",
    "        # dim: out_c x 8 x 8\n",
    "        layers += gen_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.output_size * 2\n",
    "        \n",
    "        # dim: out_c x 16 x 16\n",
    "        layers += gen_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.output_size\n",
    "        \n",
    "        # dim: out_c x 32 x 32\n",
    "        layers += gen_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.channels\n",
    "        \n",
    "        # dim: out_c x 64 x 64\n",
    "        layers += [nn.ConvTranspose2d(in_c, out_c, 4, 2, 1), nn.Tanh()]\n",
    "        \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-476a8b259e60>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-476a8b259e60>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    def Discriminator(nn.Module):\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def discrim_block(input_channels, output_channels, kernel_size, stride, padding):\n",
    "    layers = []\n",
    "    layers += [nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "    layers += [nn.BatchNorm2d(output_channels)]\n",
    "    layers += [nn.LeakyReLU(0.2, inplace=True)]\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3, input_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = build_layers()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze()\n",
    "    \n",
    "    def build_layers(self):\n",
    "        layers = []\n",
    "        in_c = self.channels\n",
    "        out_c = self.input_dim\n",
    "        \n",
    "        # dim: out_c x 32 x 32\n",
    "        layers += discrim_block(in_c, out_c, 4, 1, 0)\n",
    "        in_c = out_c\n",
    "        out_c = self.input_dim * 2\n",
    "        \n",
    "        # dim: out_c x 16 x 16\n",
    "        layers += discrim_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.input_dim * 4\n",
    "        \n",
    "        # dim: out_c x 8 x 8\n",
    "        layers += discrim_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.input_dim * 8\n",
    "        \n",
    "        # dim: out_c x 4 x 4\n",
    "        layers += discrim_block(in_c, out_c, 4, 2, 1)\n",
    "        in_c = out_c\n",
    "        out_c = self.channels\n",
    "        \n",
    "        # dim: out_c x 64 x 64\n",
    "        layers += [nn.Conv2d(in_c, 1, 4, 1, 0), nn.Sigmoid()]\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = 100\n",
    "gen_output = 64\n",
    "\n",
    "gen = Generator(input_size=gen_input, output_dim=gen_output)\n",
    "discrim = Discriminator()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device: {}\".format(device))\n",
    "gen.to(device)\n",
    "discrim.to(device)\n",
    "\n",
    "learn_rate = 0.001\n",
    "\n",
    "optG = optim.Adam(gen.parameters(), lr=learn_rate)\n",
    "optD = optim.Adam(discrim.parameters(), lr=learn_rate)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(gen_output, gen_input, 1, 1, device=device)\n",
    "\n",
    "real_label = 0.9\n",
    "fake_label = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
